{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yelp Data Challenge - NLP\n",
    "\n",
    "\n",
    "Nov 3rd 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: I processed review data from 2016-01-01 as stated in Data_preprocessing file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('last_2_year_restaurant_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>name</th>\n",
       "      <th>categories</th>\n",
       "      <th>ave_stars</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>--9e1ONYQuAa-CB_Rrw7Tw</td>\n",
       "      <td>Delmonico Steakhouse</td>\n",
       "      <td>[Steakhouses, Cajun/Creole, Restaurants]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-01-20</td>\n",
       "      <td>0</td>\n",
       "      <td>L8lo5SKXfZRlbn1bpPiC9w</td>\n",
       "      <td>5</td>\n",
       "      <td>Went here for guys weekend. Unbelievable. Ravi...</td>\n",
       "      <td>0</td>\n",
       "      <td>nT8zgjoc-PbdBoQsFEXFLw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>--9e1ONYQuAa-CB_Rrw7Tw</td>\n",
       "      <td>Delmonico Steakhouse</td>\n",
       "      <td>[Steakhouses, Cajun/Creole, Restaurants]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-02-12</td>\n",
       "      <td>0</td>\n",
       "      <td>3cnTdE45VrsS0o4cVhfGog</td>\n",
       "      <td>3</td>\n",
       "      <td>Located inside my favorite hotel Venetian, Del...</td>\n",
       "      <td>1</td>\n",
       "      <td>rOIrilMC7VFwFVBeQNiKMw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>--9e1ONYQuAa-CB_Rrw7Tw</td>\n",
       "      <td>Delmonico Steakhouse</td>\n",
       "      <td>[Steakhouses, Cajun/Creole, Restaurants]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-04-18</td>\n",
       "      <td>0</td>\n",
       "      <td>QtLQQlmFINUSb2K_gE7J1Q</td>\n",
       "      <td>4</td>\n",
       "      <td>Great food, great service. Expect to pay a pre...</td>\n",
       "      <td>1</td>\n",
       "      <td>ez3GBw83OIgzzgvc0R4jzw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>--9e1ONYQuAa-CB_Rrw7Tw</td>\n",
       "      <td>Delmonico Steakhouse</td>\n",
       "      <td>[Steakhouses, Cajun/Creole, Restaurants]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-01-03</td>\n",
       "      <td>0</td>\n",
       "      <td>oqQsexnfmYxRO-0NvxJN9A</td>\n",
       "      <td>5</td>\n",
       "      <td>My must stop at Vegas. Highly recommend to any...</td>\n",
       "      <td>0</td>\n",
       "      <td>gJrOPH-DSZWY_NX2j6Bugw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>--9e1ONYQuAa-CB_Rrw7Tw</td>\n",
       "      <td>Delmonico Steakhouse</td>\n",
       "      <td>[Steakhouses, Cajun/Creole, Restaurants]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-02-26</td>\n",
       "      <td>0</td>\n",
       "      <td>aw_5aKHlAzV0PSM7F92YFw</td>\n",
       "      <td>5</td>\n",
       "      <td>This place is top notch. We didn't order any a...</td>\n",
       "      <td>0</td>\n",
       "      <td>3Y_gz3wb7T5ur9FHqxBcgQ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id                  name  \\\n",
       "0  --9e1ONYQuAa-CB_Rrw7Tw  Delmonico Steakhouse   \n",
       "1  --9e1ONYQuAa-CB_Rrw7Tw  Delmonico Steakhouse   \n",
       "2  --9e1ONYQuAa-CB_Rrw7Tw  Delmonico Steakhouse   \n",
       "3  --9e1ONYQuAa-CB_Rrw7Tw  Delmonico Steakhouse   \n",
       "4  --9e1ONYQuAa-CB_Rrw7Tw  Delmonico Steakhouse   \n",
       "\n",
       "                                 categories  ave_stars  cool        date  \\\n",
       "0  [Steakhouses, Cajun/Creole, Restaurants]        4.0     0  2017-01-20   \n",
       "1  [Steakhouses, Cajun/Creole, Restaurants]        4.0     1  2017-02-12   \n",
       "2  [Steakhouses, Cajun/Creole, Restaurants]        4.0     1  2017-04-18   \n",
       "3  [Steakhouses, Cajun/Creole, Restaurants]        4.0     0  2017-01-03   \n",
       "4  [Steakhouses, Cajun/Creole, Restaurants]        4.0     0  2017-02-26   \n",
       "\n",
       "   funny               review_id  stars  \\\n",
       "0      0  L8lo5SKXfZRlbn1bpPiC9w      5   \n",
       "1      0  3cnTdE45VrsS0o4cVhfGog      3   \n",
       "2      0  QtLQQlmFINUSb2K_gE7J1Q      4   \n",
       "3      0  oqQsexnfmYxRO-0NvxJN9A      5   \n",
       "4      0  aw_5aKHlAzV0PSM7F92YFw      5   \n",
       "\n",
       "                                                text  useful  \\\n",
       "0  Went here for guys weekend. Unbelievable. Ravi...       0   \n",
       "1  Located inside my favorite hotel Venetian, Del...       1   \n",
       "2  Great food, great service. Expect to pay a pre...       1   \n",
       "3  My must stop at Vegas. Highly recommend to any...       0   \n",
       "4  This place is top notch. We didn't order any a...       0   \n",
       "\n",
       "                  user_id  \n",
       "0  nT8zgjoc-PbdBoQsFEXFLw  \n",
       "1  rOIrilMC7VFwFVBeQNiKMw  \n",
       "2  ez3GBw83OIgzzgvc0R4jzw  \n",
       "3  gJrOPH-DSZWY_NX2j6Bugw  \n",
       "4  3Y_gz3wb7T5ur9FHqxBcgQ  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define your feature variables, here is the text of the review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Take the values of the column that contains review text data, save to a variable named \"documents\"\n",
    "documents = df['text'].values # here .values convert panda.series to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dtype('O'), (110462,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect your documents, e.g. check the size, take a peek at elements of the numpy array\n",
    "documents.dtype, documents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"My must stop at Vegas. Highly recommend to anyone who appreciate a good steak. And try the BBQ shrimp as well. You won't be disappointed.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define your target variable (any categorical variable that may be meaningful)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For example, I am interested in perfect (5 stars) and imperfect (1-4 stars) rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make a column and take the values, save to a variable named \"target\"\n",
    "df['favorable'] = (df['stars'] > 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target = df['favorable'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True, False, False,  True,  True,  True,  True, False,  True, False], dtype=bool)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You may want to look at the statistic of the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.48462819793232059, 0.49976365184073973)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To be implemented\n",
    "target.mean(), target.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((110462,), (110462,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape, documents.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's create training dataset and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Documents is your X (review text), target is your y (whether it is favorable)\n",
    "# Now split the data to training set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split to documents_train, documents_test, target_train, target_test\n",
    "documents_train, documents_test, target_train, target_test = train_test_split(\n",
    "    documents,\n",
    "    target,\n",
    "    test_size = 0.8, # large test size to make training faster\n",
    "    random_state = 42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((22092,), (88370,))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_train.shape, documents_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get NLP representation of the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create TfidfVectorizer, and name it vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words = \"english\", max_features = 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with your training data\n",
    "vectors_train = vectorizer.fit_transform(documents_train).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the vocab of your tfidf\n",
    "words = vectorizer.get_feature_names()\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22092, 5000)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the trained model to transform your test data\n",
    "vectors_test = vectorizer.transform(documents_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88370, 5000)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similar review search engine\n",
    "### Input a review and return a similar review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# We will need these helper methods pretty soon\n",
    "\n",
    "def get_top_values(lst, n, labels):\n",
    "    '''\n",
    "    INPUT: LIST, INTEGER, LIST\n",
    "    OUTPUT: LIST\n",
    "\n",
    "    Given a list of values, find the indices with the highest n values.\n",
    "    Return the labels for each of these indices.\n",
    "\n",
    "    e.g.\n",
    "    lst = [7, 3, 2, 4, 1]\n",
    "    n = 2\n",
    "    labels = [\"cat\", \"dog\", \"mouse\", \"pig\", \"rabbit\"]\n",
    "    output: [\"cat\", \"pig\"]\n",
    "    '''\n",
    "    return [labels[i] for i in np.argsort(lst)[::-1][:n]]  # np.argsort by default sorts values in ascending order\n",
    "\n",
    "def get_bottom_values(lst, n, labels):\n",
    "    '''\n",
    "    INPUT: LIST, INTEGER, LIST\n",
    "    OUTPUT: LIST\n",
    "\n",
    "    Given a list of values, find the indices with the lowest n values.\n",
    "    Return the labels for each of these indices.\n",
    "\n",
    "    e.g.\n",
    "    lst = [7, 3, 2, 4, 1]\n",
    "    n = 2\n",
    "    labels = [\"cat\", \"dog\", \"mouse\", \"pig\", \"rabbit\"]\n",
    "    output: [\"mouse\", \"rabbit\"]\n",
    "    '''\n",
    "    return [labels[i] for i in np.argsort(lst)[:n]] # return a list of bottom values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's use cosine similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This place is not bad. Customer service was great. They were really quick! :) \n",
      "\n",
      "I got the chicken teriyaki bento box (comes with rice, salad, and miso soup) and it's a decent size. It filled me up. I liked it. We also got sashimi salad. That was ok. I think it would be better if they had a different dressing because the one that comes with the salad is really salty in my opinion. But other than that, it was good.\n",
      "[\"This place is not bad. Customer service was great. They were really quick! :) \\n\\nI got the chicken teriyaki bento box (comes with rice, salad, and miso soup) and it's a decent size. It filled me up. I liked it. We also got sashimi salad. That was ok. I think it would be better if they had a different dressing because the one that comes with the salad is really salty in my opinion. But other than that, it was good.\"]\n"
     ]
    }
   ],
   "source": [
    "# Draw an arbitrary review from test (unseen in training) documents\n",
    "some_random_number = 100\n",
    "search_query = documents_test[some_random_number]\n",
    "search_queries = [search_query]\n",
    "print(search_query)\n",
    "print(search_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transform the drawn review(s) to vector(s)\n",
    "vector_search_queries = vectorizer.transform(search_queries).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 5000)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_search_queries.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the similarity score(s) between vector(s) and training vectors\n",
    "similarity_scores = cosine_similarity(vector_search_queries, vectors_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 22092)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01186774,  0.02703682,  0.03445728,  0.01446918,  0.        ,\n",
       "        0.02135746,  0.00575196,  0.00943934,  0.08724556,  0.03298441])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_scores[0, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's find top 5 similar reviews\n",
    "n = 5\n",
    "returned_reviews = get_top_values(similarity_scores[0], n, documents_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our search query:\n",
      "This place is not bad. Customer service was great. They were really quick! :) \n",
      "\n",
      "I got the chicken teriyaki bento box (comes with rice, salad, and miso soup) and it's a decent size. It filled me up. I liked it. We also got sashimi salad. That was ok. I think it would be better if they had a different dressing because the one that comes with the salad is really salty in my opinion. But other than that, it was good.\n"
     ]
    }
   ],
   "source": [
    "print('Our search query:‘)\n",
    "print(search_queries[0]) # To be added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Most 5 similar reviews:\n",
      "#0:\n",
      "Great food we had the Chicken Teriyaki and the Beef Teriyaki with salad and Miso soup. This was our first time there went for Valentines.\n",
      "#1:\n",
      "Came in around 7 pm on a Sunday night. They were pretty busy, probably only had two vacant tables. I ate here once before (I think during their soft opening) & wasn't a big fan. However, my friend wanted a place where they serve bento boxes so we came here. \n",
      "\n",
      "I ordered the salmon teriyaki bento box. This is actually a very large meal. It comes with soup, salad, two pieces of shumai, salmon teriyaki, rice, few pieces of vegetable tempura, and a California roll. For $15?? That's a pretty good deal. I love the creamy dressing that they put on the salad, though I know that's probably not healthy lol. The miso soup is okay, nothing spectacular. I'm pretty sure the shumai is store bought & the same ones I have from home lol only they fry theirs & I steam mine. The salmon teriyaki is very good, not drenched in teriyaki sauce & a little crispy! \n",
      "\n",
      "The service is okay. I think it's because they were busy and our server was pretty overwhelmed. If you check in, you'll get a free mochi! I will be back for the salmon teriyaki!\n",
      "#2:\n",
      "The positives, cute atmosphere, great location and bill splitting is not a problem.\n",
      "\n",
      "The negatives would be everything else.  I ordered the lunch box with beef teriyaki and chicken karage.  It came with miso soup, salad, rice and gyoza.  I've ordered similar boxes at other restaurants so I was pleasantly surprised that the portion size seemed to match the $14 price.  The miso soup and salad were what I expected.  The chicken karage was disappointing as it had no real flavor and was extremely dry.  The beef teriyaki was not very tender.  Everything just felt like it was missing flavor.  As far as service the waitress was friendly although I never got a refill on my drink and she never inquired if I wanted one.  \n",
      "\n",
      "The price point for the rolls was high compared to other places and based on the fact that I had stomach issues after this meal I won't be coming back.\n",
      "#3:\n",
      "Delicious sashimi as always! There good here is always amazing and they have three best Miso soup. The staff is always great and it's spotless all the time.\n",
      "#4:\n",
      "Holy COW!!!! What a discovery!! I'm clearly the last person to hear about this place based on the plethora of incredible reviews. \n",
      "\n",
      "My boyfriend and I came here yesterday for lunch after I scoped it on Yelp. The boyfriend almost* vetoed it, because \"how good can a bento box really be?\"\n",
      "\n",
      "It can be INCREDIBLE! \n",
      "\n",
      "He had the steak bento with the salad, brown rice and veggies. I opted for the chicken breast, salad, veggies and brown rice.\n",
      "\n",
      "The steak here is incredible. Seriously. I know what you're thinking... c'mon Laney, how good could it possibly be? Well... it's tender, not chewy, not stringy, and tastes like the ribeye at Hakkasan. Boom.\n",
      "\n",
      "The chicken; also fantastic!! It comes cooked in garlic and is all white meat with no chewy-tendon-y pieces. \n",
      "\n",
      "The brown rice was perfectly cooked, the veggies were fantastic-- not mushy or weird-- super fresh. \n",
      "\n",
      "Couple the fantastic meal with the fact that they have an entire sauce bar and I'm in my happy place. The sauce bar has teriyaki, spicy teriyaki, spicy mayo, yum yum sauce, creamy miso, wasabi dressing and more.\n",
      "\n",
      "Some people complain about the pricing at this place, but it's really not that bad! Wouldn't you rather pay a couple dollars extra for an incredible meal, rather than get a cheap crap meal? Life's too short for bad food!\n",
      "\n",
      "*Will be coming here again, and again, and again!!\n"
     ]
    }
   ],
   "source": [
    "print('\\nMost %s similar reviews:' % n)\n",
    "for i, review in enumerate(returned_reviews):\n",
    "    print('#%s:' % i) # print the similarity ranking\n",
    "    print(review) # print the review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The result makes sense** The input review (review_query) is for a Japanese restaurant and the main dish is \"chicken teriyaki bento box\". The reviews we found in the train documents are all for Japanese restaurants and \"teriyaki\"  and \"bento box\" appears in most reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying positive/negative review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive-Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a Naive-Bayes Classifier\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model_nb = MultinomialNB()\n",
    "model_nb.fit(vectors_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82821835958718093"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get score for training set\n",
    "model_nb.score(vectors_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80813624533212625"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get score for test set\n",
    "model_nb.score(vectors_test, target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a Logistic Regression Classifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model_lrc = LogisticRegression()\n",
    "model_lrc.fit(vectors_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86080934274850629"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get score for training set\n",
    "model_lrc.score(vectors_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82422767907660965"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get score for test set\n",
    "model_lrc.score(vectors_test, target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.90888943,  0.19534017,  0.39365173, ...,  0.19473474,\n",
       "       -0.15943231,  0.49773404])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lrc.coef_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q: What are the key features(words) that make the positive prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'amazing',\n",
       " u'best',\n",
       " u'delicious',\n",
       " u'awesome',\n",
       " u'great',\n",
       " u'thank',\n",
       " u'perfect',\n",
       " u'love',\n",
       " u'highly',\n",
       " u'excellent',\n",
       " u'fantastic',\n",
       " u'gem',\n",
       " u'wonderful',\n",
       " u'vegas',\n",
       " u'favorite',\n",
       " u'outstanding',\n",
       " u'bomb',\n",
       " u'wow',\n",
       " u'incredible',\n",
       " u'superb']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's find it out by ranking\n",
    "n = 20\n",
    "get_top_values(model_lrc.coef_[0], n, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.63802546,  4.53400555,  4.36808379,  3.90244479,  3.72657845,\n",
       "        3.67398152,  3.48741831,  3.46438169,  3.43399677])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The coefficients for top 20 features(words) contribute to positive prediction\n",
    "model_lrc.coef_[0][np.argsort(model_lrc.coef_[0])[::-1][1:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: The top 20 words contributing to 5-star reviews are all very positive words to describe the dishes and/or the restaurants. They have more positive weights in the LOR (log(odd)) term to make it larger. This makes sense as these words indicate that the reviewers are very satisfied with the experience in the restaurants. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q: What are the key features(words) that make the negative prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'ok',\n",
       " u'slow',\n",
       " u'average',\n",
       " u'worst',\n",
       " u'bland',\n",
       " u'okay',\n",
       " u'horrible',\n",
       " u'rude',\n",
       " u'decent',\n",
       " u'terrible',\n",
       " u'disappointing',\n",
       " u'reason',\n",
       " u'bad',\n",
       " u'mediocre',\n",
       " u'dry',\n",
       " u'stars',\n",
       " u'overpriced',\n",
       " u'poor',\n",
       " u'wasn',\n",
       " u'didn']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's find it out by ranking\n",
    "n = 20\n",
    "get_bottom_values(model_lrc.coef_[0], n, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.56412264, -4.55411089, -4.49064088, -4.2868617 , -4.16567211,\n",
       "       -4.1526603 , -3.96408732, -3.87967873, -3.72603531])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The coefficients for top 20 features(words) contribute to negative prediction\n",
    "model_lrc.coef_[0][np.argsort(model_lrc.coef_[0])[::1][1:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: The last 20 words contributing to 5-star reviews are almost all negative words. They have more negative weights in the LOR (log(odd)) term to make it smaller. These words indicate that the customers are not very satisfied with the service/dishes/environments and there is still room for the restaurants to improve. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=10, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=15, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a Random Forest Classifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model_rfc = RandomForestClassifier(max_depth = None,\n",
    "                                   n_estimators = 15,\n",
    "                                   min_samples_leaf = 10)\n",
    "\n",
    "model_rfc.fit(vectors_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82862574687669743"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get score for training set\n",
    "model_rfc.score(vectors_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78164535475840213"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get score for test set\n",
    "model_rfc.score(vectors_test, target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q: What do you see from the training score and the test score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: Random Forest model gives relatively low scores on the test data compared to Naive Bayes and Logistic Regression. As I increase n_estimators from 5 to 15(number of tress in the Random Forest model), the test score slowly increases from 0.76, to 0.78, still much lower compared to Naive Bayes and Logistic regression.\n",
    "\n",
    "But increasing n_estimators also increase the computation time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q: Can you tell what features (words) are important by inspecting the RFC model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'amazing',\n",
       " u'great',\n",
       " u'best',\n",
       " u'delicious',\n",
       " u'bad',\n",
       " u'vegas',\n",
       " u'don',\n",
       " u'love',\n",
       " u'didn',\n",
       " u'pretty',\n",
       " u'ok',\n",
       " u'rude',\n",
       " u'worst',\n",
       " u'awesome',\n",
       " u'definitely',\n",
       " u'perfect',\n",
       " u'minutes',\n",
       " u'said',\n",
       " u'wasn',\n",
       " u'good']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 20\n",
    "get_top_values(model_rfc.feature_importances_, n, words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment: Unlike Logistic Regression, Random Forest tree model gives the most impotant 20 features, either positive or negative. Comparing this to Logistic regression, the advantage of Logistic regression is clear: it is easy to inteprete the coefficients. \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cross validation\n",
    "\n",
    "[sklearn cross validation](http://scikit-learn.org/stable/modules/cross_validation.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation for Logistic regression model\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "cv_scores = cross_val_score(model_lrc,\n",
    "                            vectors_train,\n",
    "                            target_train,\n",
    "                            cv = 5,\n",
    "                            scoring = \"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores_lrc = cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy scores from cross validation for Logistic Regression model:\n",
      "[ 0.81059063  0.82032134  0.82688391  0.82684473  0.81684401]\n",
      "Std for the scores for Logistic Regression model:\n",
      "0.00620318334356\n"
     ]
    }
   ],
   "source": [
    "print ('Accuracy scores from cross validation for Logistic Regression model:')\n",
    "print (cv_scores_lrc)\n",
    "print ('Std for the scores for Logistic Regression model:')\n",
    "print (cv_scores_lrc.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation for Naive Bayes model\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "cv_scores = cross_val_score(model_nb,\n",
    "                            vectors_train,\n",
    "                            target_train,\n",
    "                            cv = 5,\n",
    "                            scoring = \"accuracy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores_nb = cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy scores from cross validation for Naive Bayes model:\n",
      "[ 0.79882326  0.80810138  0.8119484   0.80828429  0.80167535]\n",
      "Std for the scores for Naive Bayes model:\n",
      "0.00479485078044\n"
     ]
    }
   ],
   "source": [
    "print ('Accuracy scores from cross validation for Naive Bayes model:')\n",
    "print (cv_scores_nb)\n",
    "print ('Std for the scores for Naive Bayes model:')\n",
    "print (cv_scores_nb.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cross validation for Random Forest model\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "cv_scores = cross_val_score(model_rfc,\n",
    "                            vectors_train,\n",
    "                            target_train,\n",
    "                            cv = 5,\n",
    "                            scoring = \"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv_scores_rfc = cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy scores from cross validation for Random Forest model:\n",
      "[ 0.77370446  0.77212039  0.79361847  0.77908556  0.77722436]\n",
      "Std for the scores for Random Forest model:\n",
      "0.00764353443805\n"
     ]
    }
   ],
   "source": [
    "print ('Accuracy scores from cross validation for Random Forest model:')\n",
    "print (cv_scores_rfc)\n",
    "print ('Std for the scores for Random Forest model:')\n",
    "print (cv_scores_rfc.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looks like the Logistic Regression is the best classifier.\n",
    "## Do grid search for Logistic Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use grid search to find best predictable classifier\n",
    "\n",
    "\n",
    "[sklearn grid search tutorial (with cross validation)](http://scikit-learn.org/stable/modules/grid_search.html#grid-search)\n",
    "\n",
    "[sklearn grid search documentation (with cross validation)](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for accuracy\n",
      "\n",
      "\n",
      "Best parameters set found on \n",
      "{'penalty': 'l2', 'C': 1.0}\n",
      "\n",
      "Grid scores on development set:\n",
      "0.520 (+/-0.000) for {'penalty': 'l1', 'C': 0.10000000000000001}\n",
      "0.520 (+/-0.000) for {'penalty': 'l1', 'C': 0.20000000000000001}\n",
      "0.520 (+/-0.000) for {'penalty': 'l1', 'C': 0.30000000000000004}\n",
      "0.586 (+/-0.027) for {'penalty': 'l1', 'C': 0.40000000000000002}\n",
      "0.622 (+/-0.050) for {'penalty': 'l1', 'C': 0.5}\n",
      "0.624 (+/-0.045) for {'penalty': 'l1', 'C': 0.59999999999999998}\n",
      "0.632 (+/-0.034) for {'penalty': 'l1', 'C': 0.70000000000000007}\n",
      "0.652 (+/-0.046) for {'penalty': 'l1', 'C': 0.80000000000000004}\n",
      "0.656 (+/-0.048) for {'penalty': 'l1', 'C': 0.90000000000000002}\n",
      "0.654 (+/-0.055) for {'penalty': 'l1', 'C': 1.0}\n",
      "0.684 (+/-0.065) for {'penalty': 'l1', 'C': 2.0}\n",
      "0.684 (+/-0.082) for {'penalty': 'l1', 'C': 3.0}\n",
      "0.688 (+/-0.085) for {'penalty': 'l1', 'C': 4.0}\n",
      "0.680 (+/-0.077) for {'penalty': 'l1', 'C': 5.0}\n",
      "0.680 (+/-0.077) for {'penalty': 'l1', 'C': 6.0}\n",
      "0.694 (+/-0.084) for {'penalty': 'l1', 'C': 7.0}\n",
      "0.690 (+/-0.073) for {'penalty': 'l1', 'C': 8.0}\n",
      "0.684 (+/-0.081) for {'penalty': 'l1', 'C': 9.0}\n",
      "0.674 (+/-0.047) for {'penalty': 'l1', 'C': 10.0}\n",
      "0.620 (+/-0.038) for {'penalty': 'l2', 'C': 0.10000000000000001}\n",
      "0.696 (+/-0.041) for {'penalty': 'l2', 'C': 0.20000000000000001}\n",
      "0.724 (+/-0.041) for {'penalty': 'l2', 'C': 0.30000000000000004}\n",
      "0.728 (+/-0.053) for {'penalty': 'l2', 'C': 0.40000000000000002}\n",
      "0.738 (+/-0.061) for {'penalty': 'l2', 'C': 0.5}\n",
      "0.740 (+/-0.072) for {'penalty': 'l2', 'C': 0.59999999999999998}\n",
      "0.740 (+/-0.072) for {'penalty': 'l2', 'C': 0.70000000000000007}\n",
      "0.740 (+/-0.067) for {'penalty': 'l2', 'C': 0.80000000000000004}\n",
      "0.744 (+/-0.072) for {'penalty': 'l2', 'C': 0.90000000000000002}\n",
      "0.748 (+/-0.077) for {'penalty': 'l2', 'C': 1.0}\n",
      "0.746 (+/-0.052) for {'penalty': 'l2', 'C': 2.0}\n",
      "0.746 (+/-0.045) for {'penalty': 'l2', 'C': 3.0}\n",
      "0.738 (+/-0.059) for {'penalty': 'l2', 'C': 4.0}\n",
      "0.738 (+/-0.069) for {'penalty': 'l2', 'C': 5.0}\n",
      "0.736 (+/-0.068) for {'penalty': 'l2', 'C': 6.0}\n",
      "0.732 (+/-0.066) for {'penalty': 'l2', 'C': 7.0}\n",
      "0.730 (+/-0.073) for {'penalty': 'l2', 'C': 8.0}\n",
      "0.730 (+/-0.073) for {'penalty': 'l2', 'C': 9.0}\n",
      "0.728 (+/-0.079) for {'penalty': 'l2', 'C': 10.0}\n",
      "\n",
      "Detailed classificationi report:\n",
      "\n",
      "The model is strained on the full develpment set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.73      0.83      0.78     45445\n",
      "       True       0.79      0.68      0.73     42925\n",
      "\n",
      "avg / total       0.76      0.76      0.75     88370\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's turn Logistic Regression Regularization parameter C and different penalty \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "C1 = np.linspace(0.1, 1, 10)\n",
    "C2 = np.linspace(2, 10, 9)\n",
    "\n",
    "param_grid = [{'penalty': ['l1'], 'C': np.append(C1, C2)},\n",
    "              {'penalty': ['l2'], 'C': np.append(C1, C2)}]\n",
    "\n",
    "scores = ['accuracy']\n",
    "\n",
    "for score in scores:\n",
    "    print (\"# Tuning hyper-parameters for %s\" % score + \"\\n\\n\")\n",
    "    clf = GridSearchCV(LogisticRegression(),\n",
    "                       param_grid,\n",
    "                       cv = 5,\n",
    "                       scoring = score)\n",
    "    clf.fit(vectors_train[:500,:], target_train[:500])\n",
    "    print (\"Best parameters set found on \")\n",
    "    print (clf.best_params_)\n",
    "    print (\"\\nGrid scores on development set:\")\n",
    "    means = clf.cv_results_[\"mean_test_score\"]\n",
    "    stds = clf.cv_results_[\"std_test_score\"]\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.3f) for %r\"\n",
    "               % (mean, std*2, params))\n",
    "    \n",
    "    print (\"\\nDetailed classificationi report:\\n\")\n",
    "    print (\"The model is strained on the full develpment set.\")\n",
    "    print (\"The scores are computed on the full evaluation set.\")\n",
    "    print (\"\\n\")\n",
    "    y_true, y_pred = target_test, clf.predict(vectors_test)\n",
    "    print (classification_report(y_true, y_pred))\n",
    "    print (\"\\n\")\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
